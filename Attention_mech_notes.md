# intro attention mechanism transformer
core to llm model
translate: the cat ate the mouse, 
encoder / decoder
- one word at a time
- does not align with target language
- attention mechanism, technique, focus on specific part
- assign weights to most important partns
## RNN model
- only hidden state is passed to decoder
- sequence to sequence
## Attention Network Model
- more data passed (parameters)
- more context, all hidden state
- extra step before final output
## Extra Step in Decoder
1. look at encoder hidden state
2. Softmax score (of weight) CONTEXT_VECTOR weight_sum
3. Amplify hidden state with high score.
4. Attention is the layer of probability weight.
Concatenated_vector of many weighted sums,
Passed through a feed_forward neural_network
trained-jointly with model - TO OUTPUT NEXT WORD.
hidden state AND context vector used to calculate weight sum.
concatenated into one vector.
Output NEXT_WORD of TIMESTEP. as ATTENTION, in decoder.
Process continues for EACH TOKEN, until end of sentence.
GENERATED by DECODER.
ATTENTION MECHANISM - improves traditional encoder/decoder.

